{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eax_pG-ls9Gk",
        "outputId": "672ecefe-2ee6-46f0-9568-caa51c7e1c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nPlz-xGr--D",
        "outputId": "5d892a90-f2f7-4365-e0a7-aab6736070ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/PRIM/code/src\n",
            "Preprocess module imported successfully!\n",
            "model_layers module imported successfully!\n",
            "losses module imported successfully!\n",
            "split module imported successfully!\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/PRIM/code/src\n",
        "%run setup_imports.py\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQoNlE17r--B"
      },
      "source": [
        "# 1. PREPROCESS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tULJsyrHr--F"
      },
      "outputs": [],
      "source": [
        "# Preprocessing input data\n",
        "input_folder = \"/content/drive/MyDrive/PRIM/dataset/inputs\"\n",
        "output_folder = \"/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith(\".xml\"):\n",
        "        input_xml_path = os.path.join(input_folder, file_name)\n",
        "        print(f\"Processing: {input_xml_path}\")\n",
        "        tree = preprocess.load_xml(input_xml_path)\n",
        "        extracted_data = preprocess.extract_data_from_input(tree)\n",
        "        df = preprocess.data_to_dataframe(extracted_data)\n",
        "        df_interpolated = preprocess.interpolate_data_with_flag(df)\n",
        "        df_normalized = preprocess.normalize(df_interpolated)\n",
        "        output_csv_path = os.path.join(output_folder, file_name.replace(\".xml\", \"_preprocessed.csv\"))\n",
        "        preprocess.save_data(df_normalized, output_csv_path)\n",
        "\n",
        "        print(f\"Saved preprocessed file to: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJiAXsTOr--G"
      },
      "outputs": [],
      "source": [
        "# Preprocessing output data\n",
        "import numpy as np\n",
        "input_folder = \"/content/drive/MyDrive/PRIM/dataset/outputs\"\n",
        "output_folder = \"/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "desired_columns = [\n",
        "    'frame', 'stroke', 'player 1', 'player 2',\n",
        "    'backhand', 'forehand', 'serve', 'ball pass',\n",
        "    'point', 'mistake', 'void serve'\n",
        "]\n",
        "\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith(\".xml\"):\n",
        "        input_xml_path = os.path.join(input_folder, file_name)\n",
        "        print(f\"Processing: {input_xml_path}\")\n",
        "        tree = preprocess.load_xml(input_xml_path)\n",
        "        extracted_data = preprocess.extract_data_from_output(tree)\n",
        "        df = preprocess.data_to_dataframe(extracted_data)\n",
        "        df['stroke'] = np.where(df['player 1'] + df['player 2'] <= 1, df['player 1'] + df['player 2'], 1)\n",
        "        df['void serve'] = np.where(df['let serve'] + df['void serve'] <= 1, df['let serve'] + df['void serve'], 1)\n",
        "        df = df[desired_columns]\n",
        "        df['player'] = np.where(df['player 1'] == 1, 1, 0)\n",
        "        df['type'] = np.where(df['backhand'] == 1, 1, 0)\n",
        "        df['role'] = np.where(df['serve'] == 1, 1, np.where(df['ball pass'] == 1, 2, 0))\n",
        "        df['impact'] = np.where(df['point'] == 1, 1, np.where(df['mistake'] == 1, 2, np.where(df['void serve'] == 1, 3, 0)))\n",
        "        result = df[['frame', 'stroke', 'player', 'type', 'role', 'impact']]\n",
        "        output_csv_path = os.path.join(output_folder, file_name.replace(\".xml\", \"_preprocessed.csv\"))\n",
        "        preprocess.save_data(result, output_csv_path)\n",
        "        print(f\"Saved preprocessed file to: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3OFPx3C59WD"
      },
      "source": [
        "# 2. LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8O9PA_vvJXA"
      },
      "outputs": [],
      "source": [
        "feature_dir = \"/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed\"\n",
        "target_dir = \"/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed\"\n",
        "\n",
        "feature_files = [\n",
        "    os.path.join(feature_dir, f) for f in os.listdir(feature_dir)\n",
        "    if os.path.isfile(os.path.join(feature_dir, f))\n",
        "]\n",
        "\n",
        "target_files = [\n",
        "    os.path.join(target_dir, f) for f in os.listdir(target_dir)\n",
        "    if os.path.isfile(os.path.join(target_dir, f))\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_dbH6bmxBQi",
        "outputId": "51e84939-13de-4cab-b3b3-8f93e7a4531a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Features: 565060 27\n",
            "Validation Features: 85558 4\n",
            "Test Features: 0 0\n"
          ]
        }
      ],
      "source": [
        "feature_files.sort()\n",
        "target_files.sort()\n",
        "train_features, train_targets, val_features, val_targets, test_features, test_targets = proportional_split(feature_files, target_files, calculate_file_lengths(feature_files), train_ratio=0.9, val_ratio=0.1, test_ratio=0)\n",
        "print(\"Train Features:\", sum(calculate_file_lengths(train_features)), len(train_features))\n",
        "print(\"Validation Features:\", sum(calculate_file_lengths(val_features)), len(val_features))\n",
        "print(\"Test Features:\", sum(calculate_file_lengths(test_features)), len(test_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X8C3T6lzHnY"
      },
      "outputs": [],
      "source": [
        "from dataset import TimeSeriesDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "chunk_length = 128\n",
        "overlap = 112\n",
        "train_dataset = TimeSeriesDataset(train_features, train_targets, chunk_length=chunk_length, overlap=0, augment= True, augment_prob=1)\n",
        "val_dataset = TimeSeriesDataset(val_features, val_targets, chunk_length=chunk_length, overlap=overlap)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSh3Zd3p0NS6",
        "outputId": "67843dc5-3b1b-4a4a-9705-9f693ef8bfb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 128, 116])\n",
            "torch.Size([16, 128, 5])\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, (features, targets) in enumerate(train_dataloader):\n",
        "  print(features.shape)\n",
        "  print(targets.shape)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zeofr1vusaR1",
        "outputId": "386b9e12-830b-48dc-dedc-430e1a96a7e0"
      },
      "outputs": [],
      "source": [
        "# tinsach tifsakh hadha\n",
        "import wandb\n",
        "wandb.login(key=\"83c00bb6bf15c1285e4617518f9a3c0b65a13872\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUvr6TyVsOia"
      },
      "source": [
        "# 2. Experiment nÂ°1: regular_loss + baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXeB35f3suKO"
      },
      "outputs": [],
      "source": [
        "#baseline model\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, hidden_dim, num_layers, dropout=0.1,max_len=1024):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder = TransformerEncoder(\n",
        "            input_dim=input_dim,\n",
        "            num_heads=num_heads,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            max_len=max_len\n",
        "        )\n",
        "\n",
        "        # Task-specific heads\n",
        "        self.stroke_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "        # Player classification head (binary)\n",
        "        self.player_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 2)           # Output layer\n",
        "        )\n",
        "\n",
        "        # Type classification head (binary)\n",
        "        self.type_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 2)           # Output layer\n",
        "        )\n",
        "\n",
        "        # Role classification head (3-class)\n",
        "        self.role_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3)           # Output layer\n",
        "        )\n",
        "\n",
        "        # Impact classification head (4-class)\n",
        "        self.impact_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features using the transformer encoder\n",
        "        features = self.encoder(x)\n",
        "\n",
        "        # Task-specific predictions\n",
        "        stroke_pred = self.stroke_head(features)\n",
        "        player_pred = self.player_head(features)\n",
        "        type_pred = self.type_head(features)\n",
        "        role_pred = self.role_head(features)\n",
        "        impact_pred = self.impact_head(features)\n",
        "\n",
        "        return stroke_pred, player_pred, type_pred, role_pred, impact_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aYltNfNVulKK",
        "outputId": "a8577f80-051a-45f3-db1d-b4b908540491"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TransformerClassifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4b64d9bfdca0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = TransformerClassifier(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m124\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TransformerClassifier' is not defined"
          ]
        }
      ],
      "source": [
        "from train import train\n",
        "model = TransformerClassifier(\n",
        "    input_dim=124,\n",
        "    num_heads=3,\n",
        "    hidden_dim=192,\n",
        "    num_layers=4,\n",
        "    dropout=0.1,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "loss_fn = regular_loss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train and validate\n",
        "train(\n",
        "    model=model,\n",
        "    params=model.parameters(),\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=100,\n",
        "    device=device,\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqPB5fdx6Lo4",
        "outputId": "d451e982-f959-4188-80c9-2eebc36480ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 1805772\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWJ4Jwp-_KI8"
      },
      "source": [
        "# 3. Experiment nÂ°2: weighted loss + baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jS1MBhEAsG5"
      },
      "outputs": [],
      "source": [
        "from train import train\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    input_dim=124,\n",
        "    num_heads=3,\n",
        "    hidden_dim=192,\n",
        "    num_layers=4,\n",
        "    dropout=0.1,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "loss_fn = weighted_loss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train and validate\n",
        "train(\n",
        "    model=model,\n",
        "    params=model.parameters(),\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=100,\n",
        "    device=device,\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJATbNuKg-TY"
      },
      "source": [
        "# 4. Experiment nÂ°3: multivariate weighted loss + baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aioxKunLheBh"
      },
      "outputs": [],
      "source": [
        "from train import train\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    input_dim=124,\n",
        "    num_heads=3,\n",
        "    hidden_dim=192,\n",
        "    num_layers=4,\n",
        "    dropout=0.1,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "log_vars = nn.Parameter(torch.zeros(5))\n",
        "params = list(model.parameters()) + [log_vars]\n",
        "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "loss_fn = multivariate_weighted_loss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=100,\n",
        "    device=device,\n",
        "    alpha=log_vars,\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm9oJhdwVfCC"
      },
      "source": [
        "# Experiment nÂ° : Multivariate Weighted Loss + Undersampling Majority Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk94dS97XEst"
      },
      "outputs": [],
      "source": [
        "# Undersampling Majority class\n",
        "\n",
        "input_folder_train = '/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train'\n",
        "output_folder_train = '/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train'\n",
        "input_folder = '/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed'\n",
        "output_folder = '/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed'\n",
        "\n",
        "os.makedirs(input_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "p = 0\n",
        "for file_name in os.listdir(input_folder):\n",
        "        input_csv_path = os.path.join(input_folder, file_name)\n",
        "        output_csv_path = os.path.join(output_folder, file_name)\n",
        "\n",
        "        df_input = pd.read_csv(input_csv_path)\n",
        "        df_output = pd.read_csv(output_csv_path)\n",
        "\n",
        "        n_before = 20\n",
        "        n_after = 20\n",
        "\n",
        "        stroke_indices = df_output[df_output[\"stroke\"] == 1].index\n",
        "\n",
        "        indices_to_keep = set()\n",
        "\n",
        "        for idx in stroke_indices:\n",
        "            indices_to_keep.add(idx)\n",
        "            indices_to_keep.update(range(max(0, idx - n_before), idx))\n",
        "            indices_to_keep.update(range(idx + 1, min(len(df_output), idx + 1 + n_after)))\n",
        "\n",
        "        indices_to_keep = sorted(indices_to_keep)\n",
        "        reduced_df_output = df_output.iloc[indices_to_keep].drop_duplicates(subset=['frame']).reset_index(drop=True)\n",
        "        reduced_df_input = df_input.iloc[indices_to_keep].reset_index(drop=True)\n",
        "\n",
        "        if reduced_df_input.isna().sum().sum() > 0:\n",
        "            print(f\"NaN values detected in reduced input file {file_name}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        if reduced_df_output.isna().sum().sum() > 0:\n",
        "            print(f\"NaN values detected in reduced output file {file_name}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        print(f\"Reduced Output DataFrame shape: {reduced_df_output.shape}\")\n",
        "        print(f\"Reduced Input DataFrame shape: {reduced_df_input.shape}\")\n",
        "\n",
        "        input_csv_path = os.path.join(input_folder_train, file_name)\n",
        "        output_csv_path = os.path.join(output_folder_train, file_name)\n",
        "\n",
        "        preprocess.save_data(reduced_df_input, input_csv_path)\n",
        "        preprocess.save_data(reduced_df_output, output_csv_path)\n",
        "\n",
        "        print(f\"Saved preprocessed output file to: {output_csv_path}\")\n",
        "        print(f\"Saved preprocessed input file to: {input_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11D_R-RAdUNP"
      },
      "outputs": [],
      "source": [
        "feature_dir_train = \"/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train\"\n",
        "target_dir_train = \"/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train\"\n",
        "\n",
        "feature_dir_val = \"/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed\"\n",
        "target_dir_val = \"/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed\"\n",
        "\n",
        "feature_files_train = [\n",
        "    os.path.join(feature_dir_train, f) for f in os.listdir(feature_dir_train)\n",
        "    if os.path.isfile(os.path.join(feature_dir_train, f))\n",
        "]\n",
        "\n",
        "target_files_train = [\n",
        "    os.path.join(target_dir_train, f) for f in os.listdir(target_dir_train)\n",
        "    if os.path.isfile(os.path.join(target_dir_train, f))\n",
        "]\n",
        "\n",
        "feature_files_val = [\n",
        "    os.path.join(feature_dir_val, f) for f in os.listdir(feature_dir_val)\n",
        "    if os.path.isfile(os.path.join(feature_dir_val, f))\n",
        "]\n",
        "\n",
        "target_files_val = [\n",
        "    os.path.join(target_dir_val, f) for f in os.listdir(target_dir_val)\n",
        "    if os.path.isfile(os.path.join(target_dir_val, f))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPYDkFWbdr4A",
        "outputId": "83481db1-153a-46cb-fbb6-99fe3e66c81d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Features: 128552 24\n",
            "Validation Features: 43614 1\n"
          ]
        }
      ],
      "source": [
        "feature_files_train.sort()\n",
        "target_files_train.sort()\n",
        "feature_files_val.sort()\n",
        "target_files_val.sort()\n",
        "\n",
        "train_features, train_targets, val_features, val_targets = feature_files_train[:24], target_files_train[:24], feature_files_val[24:25], target_files_val[24:25]\n",
        "print(\"Train Features:\", sum(calculate_file_lengths(train_features)), len(train_features))\n",
        "print(\"Validation Features:\", sum(calculate_file_lengths(val_features)), len(val_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeKm9Fq6gNgR"
      },
      "outputs": [],
      "source": [
        "def multivariate_weighted_loss(y_pred, y_true, log_vars):\n",
        "    \"\"\"\n",
        "    y_pred : tuple of predictions (logits)\n",
        "    y_true : tuple of ground truth labels\n",
        "    \"\"\"\n",
        "    y_pred_stroke, y_pred_player, y_pred_type, y_pred_role, y_pred_impact = y_pred\n",
        "    y_true_stroke = y_true[:, :, 0].unsqueeze(-1)\n",
        "    y_true_player = y_true[:, :, 1:3]\n",
        "    y_true_type = y_true[:, :, 3:5]\n",
        "    y_true_role = torch.stack([\n",
        "        y_true[:, :, 5],\n",
        "        y_true[:, :, 6],\n",
        "        torch.ones_like(y_true[:, :, 5]) - y_true[:, :, 5] - y_true[:, :, 6]\n",
        "    ], dim=-1)\n",
        "    y_true_impact = torch.cat([\n",
        "        y_true[:, :, 7:],\n",
        "        1 - torch.sum(y_true[:, :, 7:], dim=-1, keepdim=True)\n",
        "    ], dim=-1)\n",
        "\n",
        "    # Task-specific loss computation\n",
        "    mask_stroke = (y_true_stroke != -1).squeeze(-1)\n",
        "    #Weights are calculated based on all data using the following formula w_i = nb_total_samples / (nb_classes * nb_samples_i)\n",
        "    weight = torch.zeros_like(y_true_stroke)\n",
        "    weight[y_true_stroke == 1] = 10.57\n",
        "    weight[y_true_stroke == 0] = 0.52\n",
        "    weight = weight[mask_stroke].squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "    bce_with_logits_loss = nn.BCEWithLogitsLoss(weight=weight)\n",
        "    loss_stroke = bce_with_logits_loss(y_pred_stroke[mask_stroke].squeeze(-1), y_true_stroke[mask_stroke].squeeze(-1)) if mask_stroke.sum() > 0 else torch.tensor(0.0, device=y_pred_stroke.device)\n",
        "\n",
        "    mask = (y_true_stroke == 1).squeeze(-1)\n",
        "\n",
        "    # Player and Type tasks (No weighting applied here almost balanced)\n",
        "    loss_player = F.cross_entropy(y_pred_player[mask], y_true_player[mask]) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_player.device)\n",
        "    loss_type = F.cross_entropy(y_pred_type[mask], y_true_type[mask]) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_type.device)\n",
        "\n",
        "    weights_role = torch.tensor([1.63, 3.41, 0.47], dtype=torch.float32, device=y_pred_role.device)\n",
        "\n",
        "    loss_role = F.cross_entropy(y_pred_role[mask], y_true_role[mask], weight=weights_role) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_role.device)\n",
        "\n",
        "    weights_impact = torch.tensor([8.71, 1.42, 34.7, 0.31], dtype=torch.float32, device=y_pred_role.device)\n",
        "\n",
        "    loss_impact = F.cross_entropy(y_pred_impact[mask], y_true_impact[mask], weight=weights_impact) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_role.device)\n",
        "\n",
        "    total_loss = (\n",
        "        (1 / (2 * torch.exp(log_vars[0]))) * loss_stroke + log_vars[0] +\n",
        "        (1 / (2 * torch.exp(log_vars[1]))) * loss_player + log_vars[1] +\n",
        "        (1 / (2 * torch.exp(log_vars[2]))) * loss_type + log_vars[2] +\n",
        "        (1 / (2 * torch.exp(log_vars[3]))) * loss_role + log_vars[3] +\n",
        "        (1 / (2 * torch.exp(log_vars[4]))) * loss_impact + log_vars[4]\n",
        "    )\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToyjTmztg7va"
      },
      "outputs": [],
      "source": [
        "from dataset import TimeSeriesDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "chunk_length = 128\n",
        "overlap = 112\n",
        "train_dataset = TimeSeriesDataset(train_features, train_targets, chunk_length=chunk_length, overlap=0)\n",
        "val_dataset = TimeSeriesDataset(val_features, val_targets, chunk_length=chunk_length, overlap=overlap)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WH3uqntgr0E"
      },
      "outputs": [],
      "source": [
        "from train import train\n",
        "\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    input_dim=124,\n",
        "    num_heads=8,\n",
        "    hidden_dim=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.1,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "log_vars = nn.Parameter(torch.zeros(5))\n",
        "params = list(model.parameters()) + [log_vars]\n",
        "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "loss_fn = multivariate_weighted_loss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=100,\n",
        "    device=device,\n",
        "    alpha=log_vars,\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWJ1S8kFpq_C"
      },
      "source": [
        "# Model Hyperparameter Tuning / Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GshFqi_3FjA",
        "outputId": "2fa6a284-0380-487f-9b4f-c9f81dad71c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Features: 487571 25\n",
            "Validation Features: 77489 2\n",
            "Test Features: 85558 4\n"
          ]
        }
      ],
      "source": [
        "feature_files.sort()\n",
        "target_files.sort()\n",
        "train_features, train_targets, val_features, val_targets, test_features, test_targets = proportional_split(feature_files, target_files, calculate_file_lengths(feature_files), train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)\n",
        "print(\"Train Features:\", sum(calculate_file_lengths(train_features)), len(train_features))\n",
        "print(\"Validation Features:\", sum(calculate_file_lengths(val_features)), len(val_features))\n",
        "print(\"Test Features:\", sum(calculate_file_lengths(test_features)), len(test_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q06LdRyw3cNa"
      },
      "outputs": [],
      "source": [
        "from dataset import TimeSeriesDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "chunk_length = 128\n",
        "overlap = 112\n",
        "train_dataset = TimeSeriesDataset(train_features, train_targets, chunk_length=chunk_length, overlap=0, augment= True, augment_prob=1)\n",
        "val_dataset = TimeSeriesDataset(val_features, val_targets, chunk_length=chunk_length, overlap=overlap)\n",
        "test_dataset = TimeSeriesDataset(test_features, test_targets, chunk_length=chunk_length, overlap=overlap)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0yQJVZel1aD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for Binary and Multi-Class Classification.\n",
        "    Args:\n",
        "        gamma (float): Focusing parameter to down-weight easy examples. Default: 2.\n",
        "        alpha (float, list, or None): Balancing factor for classes.\n",
        "            - If float, applies the same alpha to the positive class (binary).\n",
        "            - If list, applies per-class weights (multi-class).\n",
        "            - If None, no weighting is applied. Default: None.\n",
        "        reduction (str): Specifies the reduction to apply to the output:\n",
        "            - 'none': No reduction.\n",
        "            - 'mean': Average over all examples.\n",
        "            - 'sum': Sum over all examples. Default: 'mean'.\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        # Process alpha\n",
        "        if isinstance(alpha, (float, int)):\n",
        "            self.alpha = torch.tensor([alpha, 1 - alpha])\n",
        "        elif isinstance(alpha, list):\n",
        "            self.alpha = torch.tensor(alpha)\n",
        "        else:\n",
        "            self.alpha = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logits (Tensor): Predicted logits (before softmax) of shape (N, C, ...) for multi-class\n",
        "                             or (N, ...) for binary classification.\n",
        "            targets (Tensor): Ground truth labels of shape (N, ...) for binary or (N, ...) for multi-class.\n",
        "        Returns:\n",
        "            Tensor: Computed focal loss.\n",
        "        \"\"\"\n",
        "        # For multi-class classification\n",
        "        if logits.dim() > 2:\n",
        "            # Apply softmax across the class dimension\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "\n",
        "            # One-hot encode targets\n",
        "            targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float()\n",
        "            ce_loss = -targets_one_hot * torch.log(probs.clamp(min=1e-6))\n",
        "            p_t = probs * targets_one_hot + (1 - probs) * (1 - targets_one_hot)\n",
        "        else:\n",
        "            probs = torch.sigmoid(logits)\n",
        "            ce_loss = -targets * torch.log(probs.clamp(min=1e-6)) - (1 - targets) * torch.log((1 - probs).clamp(min=1e-6))\n",
        "            p_t = probs * targets + (1 - probs) * (1 - targets)\n",
        "\n",
        "        focal_term = (1 - p_t) ** self.gamma\n",
        "        # Apply alpha if provided\n",
        "        if self.alpha is not None:\n",
        "            targets = targets.to(self.alpha.device).long()\n",
        "            # Index alpha\n",
        "\n",
        "            alpha_t = self.alpha[targets]\n",
        "            # Apply alpha to the loss\n",
        "            ce_loss = ce_loss.to(self.alpha.device) * alpha_t.unsqueeze(-1)\n",
        "\n",
        "        # Combine focal term and cross-entropy\n",
        "        loss = focal_term.to(self.alpha.device) * ce_loss\n",
        "        # Reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:  # 'none'\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd9p3XoZ5khF"
      },
      "outputs": [],
      "source": [
        "# dynamic class weighting\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def loss(y_pred, y_true, log_vars):\n",
        "    y_pred_stroke, y_pred_player, y_pred_type, y_pred_role, y_pred_impact = y_pred\n",
        "    y_true_stroke = y_true[:, :, 0]\n",
        "    y_true_player = y_true[:, :, 1]\n",
        "    y_true_type = y_true[:, :, 2]\n",
        "    y_true_role = y_true[:, :, 3]\n",
        "    y_true_impact = y_true[:, :, 4]\n",
        "\n",
        "    mask_stroke = (y_true_stroke != -1).squeeze(-1)\n",
        "\n",
        "    num_strokes = (y_true_stroke[mask_stroke] == 1).sum()\n",
        "    pos_weight_stroke = (mask_stroke.sum() / (num_strokes + 1e-6)).to(y_true.device) if num_strokes > 0 else torch.tensor(1.0, device=y_true.device)\n",
        "    bce_with_logits_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight_stroke)\n",
        "    loss_stroke = bce_with_logits_loss(y_pred_stroke[mask_stroke], y_true_stroke[mask_stroke]) if mask_stroke.sum() > 0 else torch.tensor(0.0, device=y_pred_stroke.device)\n",
        "\n",
        "    mask = (y_true_stroke == 1).squeeze(-1)\n",
        "\n",
        "    loss_player = F.cross_entropy(y_pred_player[mask], y_true_player[mask]) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_player.device)\n",
        "    loss_type = F.cross_entropy(y_pred_type[mask], y_true_type[mask]) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_type.device)\n",
        "\n",
        "    default_role_weight = 1.0\n",
        "    default_impact_weight = 1.0\n",
        "    if mask.sum() > 0:\n",
        "        class_counts_role = torch.bincount(y_true_role[mask].flatten(), minlength=3)\n",
        "        role_weights = (class_counts_role.sum() / (class_counts_role + 1e-6)).to(y_pred_role.device)\n",
        "        role_weights[class_counts_role == 0] = default_role_weight\n",
        "        loss_role = F.cross_entropy(y_pred_role[mask], y_true_role[mask], weight=role_weights)\n",
        "    else:\n",
        "        loss_role = torch.tensor(0.0, device=y_pred_role.device)\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        class_counts_impact = torch.bincount(y_true_impact[mask].flatten(), minlength=y_pred_impact.size(-1))\n",
        "        impact_weights = (class_counts_impact.sum() / (class_counts_impact + 1e-6)).to(y_pred_impact.device)\n",
        "        impact_weights[class_counts_impact == 0] = default_impact_weight\n",
        "\n",
        "        loss_impact = F.cross_entropy(y_pred_impact[mask], y_true_impact[mask], weight=impact_weights)\n",
        "    else:\n",
        "        loss_impact = torch.tensor(0.0, device=y_pred_impact.device)\n",
        "\n",
        "\n",
        "    total_loss = (\n",
        "        (1 / (2 * torch.exp(log_vars[0]))) * loss_stroke + log_vars[0] +\n",
        "        (1 / (2 * torch.exp(log_vars[1]))) * loss_player + log_vars[1] +\n",
        "        (1 / (2 * torch.exp(log_vars[2]))) * loss_type + log_vars[2] +\n",
        "        (1 / (2 * torch.exp(log_vars[3]))) * loss_role + log_vars[3] +\n",
        "        (1 / (2 * torch.exp(log_vars[4]))) * loss_impact + log_vars[4]\n",
        "    )\n",
        "\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn9bZHC-x6_C"
      },
      "outputs": [],
      "source": [
        "# dynamic class weighting\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def loss(y_pred, y_true, log_vars):\n",
        "    y_pred_stroke, y_pred_player, y_pred_type, y_pred_role, y_pred_impact = y_pred\n",
        "    y_true_stroke = y_true[:, :, 0]\n",
        "    y_true_player = y_true[:, :, 1]\n",
        "    y_true_type = y_true[:, :, 2]\n",
        "    y_true_role = y_true[:, :, 3]\n",
        "    y_true_impact = y_true[:, :, 4]\n",
        "    mask_stroke = (y_true_stroke != -1).squeeze(-1)\n",
        "    focal_loss = FocalLoss(gamma=2,alpha=0.01)\n",
        "    loss_stroke = focal_loss(y_pred_stroke[mask_stroke], y_true_stroke[mask_stroke])\n",
        "    mask = (y_true_stroke == 1).squeeze(-1)\n",
        "\n",
        "    loss_player = F.cross_entropy(y_pred_player[mask], y_true_player[mask]) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_player.device)\n",
        "    loss_type = F.cross_entropy(y_pred_type[mask], y_true_type[mask]) if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_type.device)\n",
        "\n",
        "    default_role_weight = 1.0\n",
        "    default_impact_weight = 1.0\n",
        "    if mask.sum() > 0:\n",
        "        class_counts_role = torch.bincount(y_true_role[mask].flatten(), minlength=3)\n",
        "        role_weights = (class_counts_role.sum() / (class_counts_role + 1e-6)).to(y_pred_role.device)\n",
        "        role_weights[class_counts_role == 0] = default_role_weight\n",
        "        loss_role = F.cross_entropy(y_pred_role[mask], y_true_role[mask], weight=role_weights)\n",
        "    else:\n",
        "        loss_role = torch.tensor(0.0, device=y_pred_role.device)\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        class_counts_impact = torch.bincount(y_true_impact[mask].flatten(), minlength=y_pred_impact.size(-1))\n",
        "        impact_weights = (class_counts_impact.sum() / (class_counts_impact + 1e-6)).to(y_pred_impact.device)\n",
        "        impact_weights[class_counts_impact == 0] = default_impact_weight\n",
        "\n",
        "        loss_impact = F.cross_entropy(y_pred_impact[mask], y_true_impact[mask], weight=impact_weights)\n",
        "    else:\n",
        "        loss_impact = torch.tensor(0.0, device=y_pred_impact.device)\n",
        "\n",
        "\n",
        "    total_loss = (\n",
        "        (1 / (2 * torch.exp(log_vars[0]))) * loss_stroke + log_vars[0] +\n",
        "        (1 / (2 * torch.exp(log_vars[1]))) * loss_player + log_vars[1] +\n",
        "        (1 / (2 * torch.exp(log_vars[2]))) * loss_type + log_vars[2] +\n",
        "        (1 / (2 * torch.exp(log_vars[3]))) * loss_role + log_vars[3] +\n",
        "        (1 / (2 * torch.exp(log_vars[4]))) * loss_impact + log_vars[4]\n",
        "    )\n",
        "\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "uYNgymYNM3w0",
        "outputId": "9a587d02-070d-42d3-d1c3-df8efec222f4"
      },
      "outputs": [],
      "source": [
        "from train import train\n",
        "model = TransformerClassifier(\n",
        "    input_dim=124,\n",
        "    num_heads=8,\n",
        "    hidden_dim=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.2,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "log_vars = nn.Parameter(torch.zeros(5))\n",
        "params = list(model.parameters()) + [log_vars]\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "loss_fn = loss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=200,\n",
        "    device=device,\n",
        "    alpha=log_vars,\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZxid9BvbMAo",
        "outputId": "887ed162-43f9-4235-b863-bb35a5f3d317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 3406158\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_WLLxpUAR7N"
      },
      "source": [
        "# 5. Experiment nÂ°4:  weighted loss + CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrpB1bjOiFwo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedCNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_channels, output_dim):\n",
        "        super(ImprovedCNNFeatureExtractor, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Layer 1\n",
        "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Layer 2\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Layer 3 (Project to output_dim)\n",
        "            nn.Conv1d(in_channels=128, out_channels=output_dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, seq_length, num_features)\n",
        "        # Transpose to (batch_size, num_features, seq_length) for CNN\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.cnn(x)  # Output shape: (batch_size, output_dim, seq_length)\n",
        "        # Transpose back to (batch_size, seq_length, output_dim) for Transformer\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, seq_length, num_features)\n",
        "        # Transpose to (batch_size, num_features, seq_length) for CNN\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.cnn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class StrongHybridModel(nn.Module):\n",
        "    def __init__(self, cnn_input_dim, cnn_output_dim, transformer_params):\n",
        "        super(StrongHybridModel, self).__init__()\n",
        "        self.cnn = ImprovedCNNFeatureExtractor(input_channels=cnn_input_dim, output_dim=cnn_output_dim)\n",
        "        self.transformer = TransformerClassifier(\n",
        "            input_dim=cnn_output_dim,\n",
        "            **transformer_params  # Pass your Transformer settings (e.g., num_heads, num_layers)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through CNN\n",
        "        x = self.cnn(x)  # Shape: (batch_size, cnn_output_dim, reduced_seq_length)\n",
        "        # Transpose back to (batch_size, reduced_seq_length, cnn_output_dim) for Transformer\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # Pass through Transformer\n",
        "        return self.transformer(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the improved hybrid model\n",
        "model = StrongHybridModel(\n",
        "    cnn_input_dim=124,       # Number of input features\n",
        "    cnn_output_dim=128,      # Number of features output by the CNN\n",
        "    transformer_params={     # Transformer-specific settings\n",
        "        \"num_heads\": 4,\n",
        "        \"hidden_dim\": 256,\n",
        "        \"num_layers\": 6,\n",
        "        \"dropout\": 0.2,      # Slightly higher dropout for regularization\n",
        "        \"max_len\": 128\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "\n",
        "# Use the weighted loss function for multi-task learning\n",
        "loss_fn = weighted_loss\n",
        "\n",
        "# Train the model\n",
        "train(\n",
        "    model=model,\n",
        "    params=model.parameters(),\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=100,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V2Gl16a3srA"
      },
      "outputs": [],
      "source": [
        "class StrongHybridModel(nn.Module):\n",
        "    def __init__(self, cnn_input_dim, cnn_output_dim, transformer_params):\n",
        "        super(StrongHybridModel, self).__init__()\n",
        "        self.cnn = ImprovedCNNFeatureExtractor(input_channels=cnn_input_dim, output_dim=cnn_output_dim)\n",
        "        self.transformer = TransformerClassifier(\n",
        "            input_dim=cnn_output_dim,\n",
        "            **transformer_params  # Pass your Transformer settings (e.g., num_heads, num_layers)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through CNN\n",
        "        x = self.cnn(x)  # Shape: (batch_size, cnn_output_dim, reduced_seq_length)\n",
        "        # Transpose back to (batch_size, reduced_seq_length, cnn_output_dim) for Transformer\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # Pass through Transformer\n",
        "        return self.transformer(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck5Im6ch3vnI"
      },
      "outputs": [],
      "source": [
        "# Instantiate the improved hybrid model\n",
        "model = StrongHybridModel(\n",
        "    cnn_input_dim=124,       # Number of input features\n",
        "    cnn_output_dim=128,      # Number of features output by the CNN\n",
        "    transformer_params={     # Transformer-specific settings\n",
        "        \"num_heads\": 4,\n",
        "        \"hidden_dim\": 256,\n",
        "        \"num_layers\": 6,\n",
        "        \"dropout\": 0.2,      # Slightly higher dropout for regularization\n",
        "        \"max_len\": 128\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "\n",
        "# Use the weighted loss function for multi-task learning\n",
        "loss_fn = weighted_loss\n",
        "\n",
        "# Train the model\n",
        "train(\n",
        "    model=model,\n",
        "    params=model.parameters(),\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=100,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1Cka0AvDfbS"
      },
      "outputs": [],
      "source": [
        "# Preprocessing output data\n",
        "import numpy as np\n",
        "input_folder = \"/content/drive/MyDrive/PRIM/dataset/outputs\"\n",
        "output_folder = \"/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "desired_columns = [\n",
        "    'frame', 'stroke', 'player 1', 'player 2',\n",
        "    'backhand', 'forehand', 'serve', 'ball pass',\n",
        "    'point', 'mistake', 'void serve'\n",
        "]\n",
        "\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith(\".xml\"):\n",
        "        input_xml_path = os.path.join(input_folder, file_name)\n",
        "        print(f\"Processing: {input_xml_path}\")\n",
        "        tree = preprocess.load_xml(input_xml_path)\n",
        "        extracted_data = preprocess.extract_data_from_output(tree)\n",
        "        df = preprocess.data_to_dataframe(extracted_data)\n",
        "        df['stroke'] = np.where(df['player 1'] + df['player 2'] <= 1, df['player 1'] + df['player 2'], 1)\n",
        "        df['void serve'] = df['let serve'] + df['void serve']\n",
        "        df = df[desired_columns]\n",
        "        df['player'] = np.where(df['player 1'] == 1, 1, np.where(df['player 2'] == 1, 2, 0))\n",
        "        df['type'] = np.where(df['backhand'] == 1, 1, np.where(df['forehand'] == 1, 2, 0))\n",
        "        df['role'] = np.where(df['serve'] == 1, 1, np.where(df['ball pass'] == 1, 2, 0))\n",
        "        df['impact'] = np.where(df['point'] == 1, 1,\n",
        "                                np.where(df['mistake'] == 1, 2,\n",
        "                                         np.where(df['void serve'] == 1, 3, 0)))\n",
        "\n",
        "        # Select columns for the result DataFrame\n",
        "        result = df[['frame', 'stroke', 'player', 'type', 'role', 'impact']]\n",
        "\n",
        "        output_csv_path = os.path.join(output_folder, file_name.replace(\".xml\", \"_preprocessed.csv\"))\n",
        "        preprocess.save_data(result, output_csv_path)\n",
        "        print(f\"Saved preprocessed file to: {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfyr4rRJSBx_",
        "outputId": "4e664c90-9c18-468d-8e01-a959d0b5f8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced Output DataFrame shape: (6585, 6)\n",
            "Reduced Input DataFrame shape: (6585, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/alatn_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/alatn_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (7476, 6)\n",
            "Reduced Input DataFrame shape: (7476, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/bnivh_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/bnivh_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (7395, 6)\n",
            "Reduced Input DataFrame shape: (7395, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/edysv_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/edysv_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (2856, 6)\n",
            "Reduced Input DataFrame shape: (2856, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/giplk_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/giplk_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (3390, 6)\n",
            "Reduced Input DataFrame shape: (3390, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/enekh_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/enekh_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (7478, 6)\n",
            "Reduced Input DataFrame shape: (7478, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/homwl_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/homwl_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (19937, 6)\n",
            "Reduced Input DataFrame shape: (19937, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/ipohn_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/ipohn_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (2277, 6)\n",
            "Reduced Input DataFrame shape: (2277, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/mdhln_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/mdhln_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (4774, 6)\n",
            "Reduced Input DataFrame shape: (4774, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/mehxb_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/mehxb_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (9314, 6)\n",
            "Reduced Input DataFrame shape: (9314, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/nrxwh_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/nrxwh_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (2451, 6)\n",
            "Reduced Input DataFrame shape: (2451, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/nxbhc_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/nxbhc_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (7286, 6)\n",
            "Reduced Input DataFrame shape: (7286, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/orsqi_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/orsqi_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (6585, 6)\n",
            "Reduced Input DataFrame shape: (6585, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/pciqj_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/pciqj_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (2376, 6)\n",
            "Reduced Input DataFrame shape: (2376, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/rtrwk_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/rtrwk_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (2709, 6)\n",
            "Reduced Input DataFrame shape: (2709, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/szgdk_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/szgdk_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (2967, 6)\n",
            "Reduced Input DataFrame shape: (2967, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/tbhzi_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/tbhzi_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (10875, 6)\n",
            "Reduced Input DataFrame shape: (10875, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/ypkjg_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/ypkjg_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (14042, 6)\n",
            "Reduced Input DataFrame shape: (14042, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/zidxm_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/zidxm_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (9906, 6)\n",
            "Reduced Input DataFrame shape: (9906, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/zsvsn_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/zsvsn_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (1303, 6)\n",
            "Reduced Input DataFrame shape: (1303, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/hjefd_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/hjefd_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (3152, 6)\n",
            "Reduced Input DataFrame shape: (3152, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/hucjj_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/hucjj_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (3442, 6)\n",
            "Reduced Input DataFrame shape: (3442, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/kbfjj_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/kbfjj_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (14292, 6)\n",
            "Reduced Input DataFrame shape: (14292, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/odsnu_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/odsnu_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (9754, 6)\n",
            "Reduced Input DataFrame shape: (9754, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/owagb_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/owagb_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (15785, 6)\n",
            "Reduced Input DataFrame shape: (15785, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/rhyqk_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/rhyqk_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (4963, 6)\n",
            "Reduced Input DataFrame shape: (4963, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/sckat_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/sckat_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (16125, 6)\n",
            "Reduced Input DataFrame shape: (16125, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/xunmo_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/xunmo_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (2558, 6)\n",
            "Reduced Input DataFrame shape: (2558, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/ygfdz_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/ygfdz_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (3133, 6)\n",
            "Reduced Input DataFrame shape: (3133, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/zbzpb_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/zbzpb_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (8353, 6)\n",
            "Reduced Input DataFrame shape: (8353, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/okfqy_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/okfqy_preprocessed.csv\n",
            "Reduced Output DataFrame shape: (15125, 6)\n",
            "Reduced Input DataFrame shape: (15125, 117)\n",
            "\n",
            "Saved preprocessed output file to: /content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train/wmrgi_preprocessed.csv\n",
            "Saved preprocessed input file to: /content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train/wmrgi_preprocessed.csv\n"
          ]
        }
      ],
      "source": [
        "# Undersampling Majority class\n",
        "\n",
        "input_folder_train = '/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train'\n",
        "output_folder_train = '/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train'\n",
        "input_folder = '/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed'\n",
        "output_folder = '/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed'\n",
        "\n",
        "os.makedirs(input_folder, exist_ok=True)\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "p = 0\n",
        "for file_name in os.listdir(input_folder):\n",
        "        input_csv_path = os.path.join(input_folder, file_name)\n",
        "        output_csv_path = os.path.join(output_folder, file_name)\n",
        "\n",
        "        df_input = pd.read_csv(input_csv_path)\n",
        "        df_output = pd.read_csv(output_csv_path)\n",
        "\n",
        "        n_before = 30\n",
        "        n_after = 30\n",
        "\n",
        "        stroke_indices = df_output[df_output[\"stroke\"] == 1].index\n",
        "\n",
        "        indices_to_keep = set()\n",
        "\n",
        "        for idx in stroke_indices:\n",
        "            indices_to_keep.add(idx)\n",
        "            indices_to_keep.update(range(max(0, idx - n_before), idx))\n",
        "            indices_to_keep.update(range(idx + 1, min(len(df_output), idx + 1 + n_after)))\n",
        "\n",
        "        indices_to_keep = sorted(indices_to_keep)\n",
        "        reduced_df_output = df_output.iloc[indices_to_keep].drop_duplicates(subset=['frame']).reset_index(drop=True)\n",
        "        reduced_df_input = df_input.iloc[indices_to_keep].reset_index(drop=True)\n",
        "\n",
        "        if reduced_df_input.isna().sum().sum() > 0:\n",
        "            print(f\"NaN values detected in reduced input file {file_name}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        if reduced_df_output.isna().sum().sum() > 0:\n",
        "            print(f\"NaN values detected in reduced output file {file_name}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        print(f\"Reduced Output DataFrame shape: {reduced_df_output.shape}\")\n",
        "        print(f\"Reduced Input DataFrame shape: {reduced_df_input.shape}\")\n",
        "\n",
        "        input_csv_path = os.path.join(input_folder_train, file_name)\n",
        "        output_csv_path = os.path.join(output_folder_train, file_name)\n",
        "\n",
        "        preprocess.save_data(reduced_df_input, input_csv_path)\n",
        "        preprocess.save_data(reduced_df_output, output_csv_path)\n",
        "        print()\n",
        "        print(f\"Saved preprocessed output file to: {output_csv_path}\")\n",
        "        print(f\"Saved preprocessed input file to: {input_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "002oKs65StLl"
      },
      "outputs": [],
      "source": [
        "feature_dir = \"/content/drive/MyDrive/PRIM/dataset/inputs_preprocessed_train\"\n",
        "target_dir = \"/content/drive/MyDrive/PRIM/dataset/outputs_preprocessed_train\"\n",
        "\n",
        "feature_files = [\n",
        "    os.path.join(feature_dir, f) for f in os.listdir(feature_dir)\n",
        "    if os.path.isfile(os.path.join(feature_dir, f))\n",
        "]\n",
        "\n",
        "target_files = [\n",
        "    os.path.join(target_dir, f) for f in os.listdir(target_dir)\n",
        "    if os.path.isfile(os.path.join(target_dir, f))\n",
        "]\n",
        "feature_files.sort()\n",
        "target_files.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0NubBSKXAo-"
      },
      "outputs": [],
      "source": [
        "train_features, train_targets = feature_files[:28] , target_files[:28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABQHmrF5HLjR"
      },
      "outputs": [],
      "source": [
        "from dataset import TimeSeriesDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "chunk_length = 128\n",
        "overlap = 112\n",
        "train_dataset = TimeSeriesDataset(train_features, train_targets, chunk_length=chunk_length, overlap=0, augment= True, augment_prob=1)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHjk-1tMUa-y",
        "outputId": "d3e3ed58-e914-4302-d555-ea2ed67f9858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregate Results:\n",
            "num_frames: 487571\n",
            "num_strokes: 5072\n",
            "num_player_1: 2550\n",
            "num_player_2: 0\n",
            "num_type_1: 2792\n",
            "num_type_2: 0\n",
            "num_impact_1: 155\n",
            "num_impact_2: 892\n",
            "num_impact_3: 38\n",
            "num_role_1: 1035\n",
            "num_role_2: 498\n"
          ]
        }
      ],
      "source": [
        "totals = {\n",
        "    'num_frames': 0,\n",
        "    'num_strokes': 0,\n",
        "    'num_player_1': 0,\n",
        "    'num_player_2': 0,\n",
        "    'num_type_1': 0,\n",
        "    'num_type_2': 0,\n",
        "    'num_impact_1': 0,\n",
        "    'num_impact_2': 0,\n",
        "    'num_impact_3': 0,\n",
        "    'num_role_1': 0,\n",
        "    'num_role_2': 0\n",
        "}\n",
        "\n",
        "# Loop through all feature and target files\n",
        "for file_path in target_files[:25]:\n",
        "    if file_path.endswith('.csv'):\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Aggregate calculations\n",
        "        totals['num_frames'] += df.shape[0]  # Total number of frames\n",
        "        totals['num_strokes'] += df[df['stroke'] == 1].shape[0]\n",
        "        totals['num_player_1'] += df[df['player'] == 1].shape[0]\n",
        "        totals['num_player_2'] += df[df['player'] == 2].shape[0]\n",
        "        totals['num_type_1'] += df[df['type'] == 1].shape[0]\n",
        "        totals['num_type_2'] += df[df['type'] == 2].shape[0]\n",
        "        totals['num_role_1'] += df[df['role'] == 1].shape[0]\n",
        "        totals['num_role_2'] += df[df['role'] == 2].shape[0]\n",
        "\n",
        "        totals['num_impact_1'] += df[df['impact'] == 1].shape[0]\n",
        "        totals['num_impact_2'] += df[df['impact'] == 2].shape[0]\n",
        "        totals['num_impact_3'] += df[df['impact'] == 3].shape[0]\n",
        "\n",
        "# Display the total counts\n",
        "print(\"Aggregate Results:\")\n",
        "for key, value in totals.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbuxx98k7vAx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for Binary and Multi-Class Classification.\n",
        "    Args:\n",
        "        gamma (float): Focusing parameter to down-weight easy examples. Default: 2.\n",
        "        alpha (float, list, or None): Balancing factor for classes.\n",
        "            - If float, applies the same alpha to the positive class (binary).\n",
        "            - If list, applies per-class weights (multi-class).\n",
        "            - If None, no weighting is applied. Default: None.\n",
        "        reduction (str): Specifies the reduction to apply to the output:\n",
        "            - 'none': No reduction.\n",
        "            - 'mean': Average over all examples.\n",
        "            - 'sum': Sum over all examples. Default: 'mean'.\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        # Process alpha\n",
        "        if isinstance(alpha, (float, int)):\n",
        "            self.alpha = torch.tensor([alpha, 1 - alpha])\n",
        "        elif isinstance(alpha, list):\n",
        "            self.alpha = torch.tensor(alpha)\n",
        "        else:\n",
        "            self.alpha = None\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logits (Tensor): Predicted logits (before softmax) of shape (N, C, ...) for multi-class\n",
        "                             or (N, ...) for binary classification.\n",
        "            targets (Tensor): Ground truth labels of shape (N, ...) for binary or (N, ...) for multi-class.\n",
        "        Returns:\n",
        "            Tensor: Computed focal loss.\n",
        "        \"\"\"\n",
        "        # For multi-class classification\n",
        "        if logits.dim() > 2:\n",
        "            # Apply softmax across the class dimension\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "\n",
        "            # One-hot encode targets\n",
        "            targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float()\n",
        "            ce_loss = -targets_one_hot * torch.log(probs.clamp(min=1e-6))\n",
        "            p_t = probs * targets_one_hot + (1 - probs) * (1 - targets_one_hot)\n",
        "        else:\n",
        "            probs = torch.sigmoid(logits)\n",
        "            ce_loss = -targets * torch.log(probs.clamp(min=1e-6)) - (1 - targets) * torch.log((1 - probs).clamp(min=1e-6))\n",
        "            p_t = probs * targets + (1 - probs) * (1 - targets)\n",
        "\n",
        "        focal_term = (1 - p_t) ** self.gamma\n",
        "        # Apply alpha if provided\n",
        "        if self.alpha is not None:\n",
        "            targets = targets.to(self.alpha.device).long()\n",
        "            # Index alpha\n",
        "\n",
        "            alpha_t = self.alpha[targets]\n",
        "            # Apply alpha to the loss\n",
        "            ce_loss = ce_loss.to(self.alpha.device) * alpha_t.unsqueeze(-1)\n",
        "\n",
        "        # Combine focal term and cross-entropy\n",
        "        loss = focal_term.to(self.alpha.device) * ce_loss\n",
        "        # Reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:  # 'none'\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vCLQHJKFYgB"
      },
      "outputs": [],
      "source": [
        "def multivariate_weighted_loss(y_pred, y_true, log_vars):\n",
        "    \"\"\"\n",
        "    y_pred : tuple of predictions (logits)\n",
        "    y_true : tuple of ground truth labels\n",
        "    \"\"\"\n",
        "    y_pred_stroke, y_pred_player, y_pred_type, y_pred_role, y_pred_impact = y_pred\n",
        "    y_true_stroke = y_true[:, :, 0]\n",
        "    y_true_player = y_true[:, :, 1]\n",
        "    y_true_type =  y_true[:, :, 2]\n",
        "    y_true_role = y_true[:, :, 3]\n",
        "    y_true_impact = y_true[:, :, 4]\n",
        "    mask = (y_true_stroke != -1).squeeze(-1)\n",
        "    #Weights are calculated based on all data using the following formula w_i = nb_total_samples / (nb_classes * nb_samples_i)\n",
        "    weight = torch.zeros_like(y_true_stroke)\n",
        "    weight[y_true_stroke == 1] = totals['num_frames'] / (2*totals['num_strokes'])\n",
        "    weight[y_true_stroke == 0] = totals['num_frames'] / (2*(totals['num_frames'] - totals['num_strokes']))\n",
        "    weight = weight[mask].squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "    bce_with_logits_loss = nn.BCEWithLogitsLoss(weight=weight, reduction=\"sum\")\n",
        "    loss_stroke = bce_with_logits_loss(y_pred_stroke[mask].squeeze(-1), y_true_stroke[mask].squeeze(-1))\n",
        "    #focal_loss = FocalLoss(alpha=0.01,gamma = 1.5 )\n",
        "    #loss_stroke = focal_loss(y_pred_stroke[mask].squeeze(-1), y_true_stroke[mask].squeeze(-1))\n",
        "    mask = (y_true_player != -1)\n",
        "    weights_player = torch.tensor([ totals['num_frames']/(3*(totals['num_frames'] - totals['num_player_1'] - totals['num_player_2'])), totals['num_frames']/(3*totals['num_player_1']) , totals['num_frames']/(3*totals['num_player_2'])], dtype=torch.float32, device=y_pred_role.device)\n",
        "\n",
        "    loss_player = F.cross_entropy(y_pred_player[mask].squeeze(), y_true_player[mask].squeeze().long(), weight=weights_player, reduction=\"sum\")\n",
        "    weights_type = torch.tensor([totals['num_frames']/ (3*(totals['num_frames'] - totals['num_type_1'] - totals['num_type_2'])), totals['num_frames']/ (3*totals['num_type_1']), totals['num_frames']/ (3*totals['num_type_2'])], dtype=torch.float32, device=y_pred_role.device)\n",
        "\n",
        "    loss_type = F.cross_entropy(y_pred_type[mask].squeeze(), y_true_type[mask].squeeze().long(), weight=weights_type, reduction=\"sum\")\n",
        "\n",
        "    weights_role = torch.tensor([totals['num_frames']/ (3*(totals['num_frames'] - totals['num_role_1'] - totals['num_role_2'])), totals['num_frames']/ (3*totals['num_role_1']) , totals['num_frames']/ (3*totals['num_role_2']) ], dtype=torch.float32, device=y_pred_role.device)\n",
        "\n",
        "    loss_role = F.cross_entropy(y_pred_role[mask].squeeze(), y_true_role[mask].squeeze().long(), weight=weights_role, reduction=\"sum\")\n",
        "    weights_impact = torch.tensor([totals['num_frames']/ (4*(totals['num_frames'] - totals['num_impact_1'] - totals['num_impact_2'] - totals['num_impact_3'])), totals['num_frames']/ (3*totals['num_impact_1']), totals['num_frames']/ (3*totals['num_impact_2']), totals['num_frames']/ (3*totals['num_impact_3'])], dtype=torch.float32, device=y_pred_role.device)\n",
        "\n",
        "    loss_impact = F.cross_entropy(y_pred_impact[mask].squeeze(), y_true_impact[mask].squeeze().long(), weight=weights_impact, reduction=\"sum\")\n",
        "    total_loss = (\n",
        "        (1 / (2 * torch.exp(log_vars[0]))) * loss_stroke + log_vars[0] +\n",
        "        (1 / (2 * torch.exp(log_vars[1]))) * loss_player + log_vars[1] +\n",
        "        (1 / (2 * torch.exp(log_vars[2]))) * loss_type + log_vars[2] +\n",
        "        (1 / (2 * torch.exp(log_vars[3]))) * loss_role + log_vars[3] +\n",
        "        (1 / (2 * torch.exp(log_vars[4]))) * loss_impact + log_vars[4]\n",
        "    )\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtpOgF4IrZvG"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "        return focal_loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb54-OPjril5"
      },
      "outputs": [],
      "source": [
        "def label_smoothing_loss(predictions, targets, smoothing=0.1):\n",
        "    num_classes = predictions.size(-1)\n",
        "    smooth_targets = (1 - smoothing) * F.one_hot(targets, num_classes) + smoothing / num_classes\n",
        "    return -torch.sum(smooth_targets * F.log_softmax(predictions, dim=-1), dim=-1).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30XaoeRyrXaK"
      },
      "outputs": [],
      "source": [
        "def multivariate_weighted_loss(y_pred, y_true, log_vars):\n",
        "    y_pred_stroke, y_pred_player, y_pred_type, y_pred_role, y_pred_impact = y_pred\n",
        "    y_true_stroke = y_true[:, :, 0]\n",
        "    y_true_player = y_true[:, :, 1]\n",
        "    y_true_type =  y_true[:, :, 2]\n",
        "    y_true_role = y_true[:, :, 3]\n",
        "    y_true_impact = y_true[:, :, 4]\n",
        "\n",
        "    mask = (y_true_stroke != -1).squeeze(-1)\n",
        "\n",
        "    # Focal Loss for Stroke\n",
        "    focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "    loss_stroke = focal_loss(y_pred_stroke[mask].squeeze(-1), y_true_stroke[mask].squeeze(-1))\n",
        "\n",
        "    print(f\"stroke{loss_stroke}\")\n",
        "\n",
        "    # Cross-Entropy with Label Smoothing for other tasks\n",
        "    label_smooth_loss = label_smoothing_loss\n",
        "    mask = (y_true_player != -1)\n",
        "\n",
        "    loss_player = label_smooth_loss(y_pred_player[mask].squeeze(), y_true_player[mask].squeeze().long())\n",
        "    print(f\"player{loss_player}\")\n",
        "    loss_type = label_smooth_loss(y_pred_type[mask].squeeze(), y_true_type[mask].squeeze().long())\n",
        "    print(f\"type{loss_type}\")\n",
        "    loss_role = label_smooth_loss(y_pred_role[mask].squeeze(), y_true_role[mask].squeeze().long())\n",
        "    print(f\"role{loss_role}\")\n",
        "    loss_impact = label_smooth_loss(y_pred_impact[mask].squeeze(), y_true_impact[mask].squeeze().long())\n",
        "    print(f\"impact{loss_impact}\")\n",
        "    # Dynamic Task Weights\n",
        "    total_loss = (\n",
        "        (1 / (2 * torch.exp(log_vars[0]))) * loss_stroke + log_vars[0] +\n",
        "        (1 / (2 * torch.exp(log_vars[1]))) * loss_player + log_vars[1] +\n",
        "        (1 / (2 * torch.exp(log_vars[2]))) * loss_type + log_vars[2] +\n",
        "        (1 / (2 * torch.exp(log_vars[3]))) * loss_role + log_vars[3] +\n",
        "        (1 / (2 * torch.exp(log_vars[4]))) * loss_impact + log_vars[4]\n",
        "    )\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omrxQJb7LJFA"
      },
      "outputs": [],
      "source": [
        "# dynamic class weighting\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def loss(y_pred, y_true, log_vars):\n",
        "    y_pred_stroke, y_pred_player, y_pred_type, y_pred_role, y_pred_impact = y_pred\n",
        "    y_true_stroke = y_true[:, :, 0].unsqueeze(-1)\n",
        "    y_true_player = y_true[:, :, 1].long()\n",
        "    y_true_type =  y_true[:, :, 2].long()\n",
        "    y_true_role = y_true[:, :, 3].long()\n",
        "    y_true_impact = y_true[:, :, 4].long()\n",
        "\n",
        "    mask_stroke = (y_true_stroke != -1).squeeze(-1)\n",
        "\n",
        "    num_strokes = (y_true_stroke[mask_stroke] == 1).sum()\n",
        "    pos_weight_stroke = (mask_stroke.sum() / (num_strokes + 1e-6)).to(y_true.device) if num_strokes > 0 else torch.tensor(1.0, device=y_true.device)\n",
        "    bce_with_logits_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight_stroke, reduction=\"sum\")\n",
        "    loss_stroke = bce_with_logits_loss(y_pred_stroke[mask_stroke], y_true_stroke[mask_stroke]) if mask_stroke.sum() > 0 else torch.tensor(0.0, device=y_pred_stroke.device)\n",
        "\n",
        "    mask = (y_true_stroke == 1).squeeze(-1)\n",
        "\n",
        "    loss_player = F.cross_entropy(y_pred_player[mask], y_true_player[mask], reduction=\"sum\") if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_player.device)\n",
        "    loss_type = F.cross_entropy(y_pred_type[mask], y_true_type[mask], reduction=\"sum\") if mask.sum() > 0 else torch.tensor(0.0, device=y_pred_type.device)\n",
        "\n",
        "    default_role_weight = 1.0\n",
        "    default_impact_weight = 1.0\n",
        "    if mask.sum() > 0:\n",
        "        class_counts_role = torch.bincount(y_true_role[mask].flatten(), minlength=3)\n",
        "        role_weights = (class_counts_role.sum() / (class_counts_role + 1e-6)).to(y_pred_role.device)\n",
        "        role_weights[class_counts_role == 0] = default_role_weight\n",
        "        loss_role = F.cross_entropy(y_pred_role[mask], y_true_role[mask], weight=role_weights, reduction=\"sum\")\n",
        "    else:\n",
        "        loss_role = torch.tensor(0.0, device=y_pred_role.device)\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        class_counts_impact = torch.bincount(y_true_impact[mask].flatten(), minlength=y_pred_impact.size(-1))\n",
        "        impact_weights = (class_counts_impact.sum() / (class_counts_impact + 1e-6)).to(y_pred_impact.device)\n",
        "        impact_weights[class_counts_impact == 0] = default_impact_weight\n",
        "\n",
        "        loss_impact = F.cross_entropy(y_pred_impact[mask], y_true_impact[mask], weight=impact_weights, reduction=\"sum\")\n",
        "    else:\n",
        "        loss_impact = torch.tensor(0.0, device=y_pred_impact.device)\n",
        "\n",
        "\n",
        "    total_loss = (\n",
        "        (1 / (2 * torch.exp(log_vars[0]))) * loss_stroke + log_vars[0] +\n",
        "        (1 / (2 * torch.exp(log_vars[1]))) * loss_player + log_vars[1] +\n",
        "        (1 / (2 * torch.exp(log_vars[2]))) * loss_type + log_vars[2] +\n",
        "        (1 / (2 * torch.exp(log_vars[3]))) * loss_role + log_vars[3] +\n",
        "        (1 / (2 * torch.exp(log_vars[4]))) * loss_impact + log_vars[4]\n",
        "    )\n",
        "\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Ex6JS1cumj"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, hidden_dim, num_layers, dropout=0.1,max_len=1024):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder = TransformerEncoder(\n",
        "            input_dim=input_dim,\n",
        "            num_heads=num_heads,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            max_len=max_len\n",
        "        )\n",
        "\n",
        "        # Task-specific heads\n",
        "        self.stroke_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, 1)\n",
        "            )\n",
        "\n",
        "        # Player classification head (binary)\n",
        "        self.player_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim) ,       # Output layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, 2)\n",
        "        )\n",
        "\n",
        "        # Type classification head (binary)\n",
        "        self.type_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim) ,          # Output layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, 2),\n",
        "        )\n",
        "\n",
        "        # Role classification head (3-class)\n",
        "        self.role_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim)  ,          # Output layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, 3)\n",
        "        )\n",
        "\n",
        "        # Impact classification head (4-class)\n",
        "        self.impact_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, 4),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features using the transformer encoder\n",
        "        features = self.encoder(x)\n",
        "\n",
        "        # Task-specific predictions\n",
        "        stroke_pred = self.stroke_head(features)\n",
        "        player_pred = self.player_head(features)\n",
        "        type_pred = self.type_head(features)\n",
        "        role_pred = self.role_head(features)\n",
        "        impact_pred = self.impact_head(features)\n",
        "\n",
        "        return stroke_pred, player_pred, type_pred, role_pred, impact_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZd3Y20JiDvM",
        "outputId": "91acf736-49c7-4cfe-ce47-92d18cb6a769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 2959756\n"
          ]
        }
      ],
      "source": [
        "model = TransformerClassifier(\n",
        "    input_dim=116,\n",
        "    num_heads=32,\n",
        "    hidden_dim=256,\n",
        "    num_layers=3,\n",
        "    dropout=0.2,\n",
        "    max_len=256\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1D01gifKhK-3",
        "outputId": "ebcf4cf0-8b03-4be0-f6ed-42f089af6fa5"
      },
      "outputs": [],
      "source": [
        "log_vars = nn.Parameter(torch.zeros(5))\n",
        "params = list(model.parameters()) + [log_vars]\n",
        "\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "loss_fn = loss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=200,\n",
        "    device=device,\n",
        "    alpha= log_vars,\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_83Y_IEWj3-a"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kVRtxBxXi7X",
        "outputId": "d9850e5f-60e8-4aa5-cf15-3e27b1ab76c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 2959756\n"
          ]
        }
      ],
      "source": [
        "model = TransformerClassifier(\n",
        "    input_dim=116,\n",
        "    num_heads=32,\n",
        "    hidden_dim=256,\n",
        "    num_layers=3,\n",
        "    dropout=0.2,\n",
        "    max_len=256\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5R_3I7VhXppK",
        "outputId": "51c0b8c8-4def-4077-8d1a-8ca6100912ff"
      },
      "outputs": [],
      "source": [
        "log_vars = nn.Parameter(torch.zeros(5))\n",
        "params = list(model.parameters()) + [log_vars]\n",
        "\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "loss_fn = multivariate_weighted_loss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    num_epochs=200,\n",
        "    device=device,\n",
        "    alpha= log_vars,\n",
        "    scheduler=scheduler,\n",
        "    overlap=overlap,\n",
        "    chunk_length=chunk_length\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zUvr6TyVsOia",
        "ZWJ4Jwp-_KI8",
        "OJATbNuKg-TY",
        "wm9oJhdwVfCC",
        "K_WLLxpUAR7N"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PRIM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
