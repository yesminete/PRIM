import torch
import torch.nn as nn


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:seq_len, :]

class TransformerEncoder(nn.Module):
    def __init__(self, input_dim, num_heads, hidden_dim, num_layers, dropout=0.1,max_len=1024):
        super(TransformerEncoder, self).__init__()
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        self.positional_encoding = FixedPositionalEncoding(hidden_dim, dropout=dropout, max_len=max_len)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            activation="relu",
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        # Input projection
        x = self.input_projection(x)
        x = self.positional_encoding(x)
        # Pass through the transformer encoder
        x = self.encoder(x)
        return x

class TransformerClassifier(nn.Module):
    def __init__(self, input_dim, num_heads, hidden_dim, num_layers, num_classes, dropout=0.1,max_len=1024):
        super(TransformerClassifier, self).__init__()

        # Transformer encoder
        self.encoder = TransformerEncoder(
            input_dim=input_dim,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            dropout=dropout,
            max_len=max_len
        )

        # Task-specific heads
        self.stroke_head = nn.Linear(hidden_dim, 1)  # Binary classification (stroke: yes/no)
        self.player_head = nn.Linear(hidden_dim, 2)  # Player classification
        self.type_head = nn.Linear(hidden_dim, 2)   # Stroke type classification
        self.role_head = nn.Linear(hidden_dim, 3)   # Stroke role classification
        self.impact_head = nn.Linear(hidden_dim, 5) # Impact classification

    def forward(self, x):
        # Extract features using the transformer encoder
        features = self.encoder(x)

        # Task-specific predictions
        stroke_pred = self.stroke_head(features)
        player_pred = self.player_head(features)
        type_pred = self.type_head(features)
        role_pred = self.role_head(features)
        impact_pred = self.impact_head(features)

        return stroke_pred, player_pred, type_pred, role_pred, impact_pred

import numpy as np
from collections import Counter

def aggregate_prediction(label_predictions, true_labels, chunk_length, overlap):
    """
    Aggregate predictions and true labels for overlapping chunks.
    
    Args:
        label_predictions (list of np.array): List of arrays containing predicted labels for each chunk.
        true_labels (list of np.array): List of arrays containing true labels for each chunk.
        chunk_length (int): Length of each chunk.
        overlap (int): Overlap between consecutive chunks.
    
    Returns:
        aggregated_predictions (np.array): Aggregated predictions for the entire sequence.
        aggregated_labels (np.array): Aggregated true labels for the entire sequence.
    """
    assert len(label_predictions) == len(true_labels), "Predictions and labels must have the same number of chunks."

    # Step size and total sequence length
    step = chunk_length - overlap
    sequence_length = (len(label_predictions) - 1) * step + chunk_length

    # Initialize arrays for aggregated predictions and labels
    aggregated_predictions = [[] for _ in range(sequence_length)]
    aggregated_labels = np.zeros((sequence_length, true_labels[0].shape[-1]))

    # Populate aggregated labels
    for i in range(sequence_length):
        chunk_idx = i // step
        offset = i % step
        aggregated_labels[i] = true_labels[chunk_idx][offset]

    # Populate aggregated predictions
    for i, chunk in enumerate(label_predictions):
        for j, pred in enumerate(chunk):
            index = j + i * step
            if index < sequence_length:
                aggregated_predictions[index].append(pred)

    # Majority voting for predictions
    aggregated_predictions = np.array([
        Counter(pred_list).most_common(1)[0][0] if pred_list else -1
        for pred_list in aggregated_predictions
    ])

    return aggregated_predictions, aggregated_labels

